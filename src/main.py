#!/usr/bin/env python3
"""
lask: A CLI tool to prompt ChatGPT and other LLMs from the terminal.
Usage:
    lask Your prompt here
This tool supports multiple LLM providers including OpenAI, Anthropic, and AWS Bedrock.
Configure your API keys and preferences in the ~/.lask-config file.

Features:
- Streaming responses: By default, responses are streamed in real-time as they're
  generated by the LLM. This can be disabled in the config with `streaming = false`.
- Markdown colorization: Responses are colorized based on Markdown formatting for
  better readability in the terminal.
"""

import sys
from typing import Union, Iterator

from src.config import LaskConfig
from src.providers import call_provider_api
from src.markdown_renderer import (
    get_markdown_renderer,
    colorize_markdown,
    colorize_streaming_chunk,
)


def main() -> None:
    """
    Main entry point for the lask CLI tool.
    Parses command line arguments, loads configuration,
    and calls the appropriate provider API.
    """
    if len(sys.argv) < 2:
        print("Usage: lask 'Your prompt here'")
        sys.exit(1)

    # Load config from file
    config = LaskConfig.load()

    # Get the prompt from command line arguments
    prompt: str = " ".join(sys.argv[1:])

    # Determine which provider to use
    provider: str = config.get("provider", "openai").lower()

    # Check if provider is supported
    if provider not in LaskConfig.SUPPORTED_PROVIDERS:
        print(
            f"Error: Unsupported provider '{provider}'. Supported providers are: {', '.join(LaskConfig.SUPPORTED_PROVIDERS)}"
        )
        sys.exit(1)

    try:
        # Call the appropriate API based on the provider using the provider modules
        result: Union[str, Iterator[str]] = call_provider_api(provider, config, prompt)

        # Get markdown renderer based on config
        markdown_renderer = get_markdown_renderer(config)

        # Handle streaming vs non-streaming responses
        if isinstance(result, str):
            # Non-streaming response - full text is returned at once
            if config.colorize_markdown:
                print("We are colorizing!")
                # Apply Markdown colorization to the full text
                colorized_result = colorize_markdown(result, config)
                print(colorized_result)
            else:
                print(result)
        else:
            # Streaming response - print chunks as they arrive in real-time
            # This provides immediate feedback as the LLM generates content
            for chunk in result:
                if config.colorize_markdown:
                    # Apply Markdown colorization to each chunk
                    colorized_chunk = colorize_streaming_chunk(chunk, markdown_renderer)
                    print(colorized_chunk, end="", flush=True)
                else:
                    # Print without colorization
                    print(chunk, end="", flush=True)

            # Reset the markdown renderer buffer
            if config.colorize_markdown:
                markdown_renderer.reset_buffer()

            print()  # Add a newline at the end of the complete response

    except ImportError as e:
        print(f"Error: {str(e)}")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
